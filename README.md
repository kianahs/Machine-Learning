# Machine Learning Projects

Welcome to the Machine Learning Projects repository! This collection of projects showcases various machine learning techniques implemented using Python, NumPy, PyTorch, and Scikit-Learn. Each project demonstrates a key concept in machine learning, from regression to classification and clustering. 

## üìå Projects Overview

### 1Ô∏è‚É£ Linear Regression on the Diabetes Dataset
- Implemented **Ordinary Least Squares (OLS) regression** to predict diabetes progression based on medical features.
- Conducted **feature selection and importance analysis** to evaluate the impact of individual predictors.
- Utilized **cross-validation** to assess model performance and avoid overfitting.
- Explored **residual analysis** to diagnose model assumptions.
- Used **Scikit-Learn's LinearRegression** module for training and evaluation.
- üìÇ Notebook: `wk3_Regression.ipynb`

### 2Ô∏è‚É£ Logistic Regression & Multiclass Classification (Iris Dataset)
- Implemented **binary and multiclass logistic regression** to classify iris species.
- Utilized **Softmax Regression** for multiclass classification.
- Optimized model parameters using **Gradient Descent & Stochastic Gradient Descent (SGD)**.
- Performed **feature scaling (standardization)** to improve convergence.
- Used **confusion matrices, precision-recall curves, and ROC-AUC scores** for evaluation.
- üìÇ Notebooks: `wk4_Classification_I.ipynb`, `wk4_Classification_I_completed.ipynb`

### 3Ô∏è‚É£ Neural Networks for Iris Classification
- Designed and trained a **fully connected feedforward neural network** using **PyTorch**.
- Implemented **backpropagation and gradient descent** for model training.
- Experimented with **activation functions** (ReLU, Sigmoid, Softmax) and **loss functions** (CrossEntropyLoss).
- Applied **batch normalization** and **dropout regularization** to prevent overfitting.
- Compared performance with traditional logistic regression.
- üìÇ Notebooks: `wk6_NN.ipynb`, `wk6_NN_completed.ipynb`

### 4Ô∏è‚É£ Gaussian Mixture Models for Vowel Classification
- Implemented **Gaussian Mixture Models (GMMs)** for unsupervised learning and density estimation.
- Used **Expectation-Maximization (EM) algorithm** for parameter optimization.
- Evaluated model performance using **log-likelihood and Bayesian Information Criterion (BIC)**.
- Visualized clusters using **Principal Component Analysis (PCA)**.
- Applied **formant frequency analysis** for vowel classification.
- üìÇ Notebook: `wk8_Density.ipynb`

## üöÄ Skills Demonstrated
‚úÖ Regression (Linear & Logistic)  
‚úÖ Neural Networks (Feedforward, PyTorch)  
‚úÖ Multiclass Classification & Softmax Regression  
‚úÖ Expectation-Maximization & Gaussian Mixture Models  
‚úÖ Feature Engineering & Data Preprocessing  
‚úÖ Model Optimization & Regularization Techniques  
‚úÖ Cross-Validation & Performance Metrics  

## üìú Reports & Documentation
- **Regression Lab Report** - Insights on linear regression analysis, residual diagnostics, and feature selection.
- **Classification & Neural Networks Report** - Covers logistic regression, multiclass classification, and neural network implementations.
- **Density Estimation Report** - Detailed exploration of Gaussian Mixture Models, clustering, and likelihood maximization.

## üîß Setup & Usage
1. Clone the repository:
   ```bash
   git clone <repository-url>
   cd Machine-Learning-main
   ```
2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
3. Run Jupyter Notebook:
   ```bash
   jupyter notebook
   ```
4. Open any `.ipynb` file and start exploring!

